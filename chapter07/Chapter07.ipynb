{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QhWnsXWkgpD"
      },
      "source": [
        "# 第7章: 単語ベクトル\n",
        "単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dIegxYdkTPh",
        "outputId": "8b74a5b2-830d-4787-9663-a6177612b186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown --fuzzy 'https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?usp=sharing&resourcekey=0-wjGZdNAUop6WykTtMip30g'\n",
        "!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSNAd4CvkVKv",
        "outputId": "936d52a3-c7fb-4646-87cd-1091ca053d2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#準備：https://qiita.com/yamaru/items/822af1f7f77666381e20\n",
        "FILE_ID = \"0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
        "FILE_NAME = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt\n"
      ],
      "metadata": {
        "id": "rBT3QTPVOr7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57363d65-45aa-4ed3-ab80-5b145a8cfd9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-25 03:52:20--  https://drive.google.com/uc?export=download&confirm=&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.218.102, 173.194.218.113, 173.194.218.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.218.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "\r          GoogleNew     [<=>                 ]       0  --.-KB/s               \rGoogleNews-vectors-     [ <=>                ]   1.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-25 03:52:20 (19.1 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1978]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the above downloads keeps failing, so i've downloaded the file onto my google drive and mounted it @/content/drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zrzmfdzCUdJ",
        "outputId": "aaa7d93c-4e87-4a4b-89d0-ad9ff94fffe0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwngqpeFkpJf"
      },
      "source": [
        "## 60. 単語ベクトルの読み込みと表示\n",
        "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/u/1/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"use gensim\n",
        "keyedVectors module allows for quick load of models, but cannot train further. https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "\n",
        "The vectors can also be instantiated from an existing file on disk in the original Google’s word2vec C format as a KeyedVectors instance\n",
        ">>>from gensim.test.utils import datapath\n",
        ">>>wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format\n",
        ">>>wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)  # C bin format\n",
        "\n",
        "*GoogleNews-vectors-negative300.bin.gz is in bin(binary) format, so use the latter code\n",
        "\"\"\"\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format('./drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "metadata": {
        "id": "NMp5w3LnQdXB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model['United_States'][:100]\n",
        "#100次元まで表示"
      ],
      "metadata": {
        "id": "21n6akFPgqzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91fc764-99fe-4b29-f1e5-5117d10d6a8c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.61328125e-02, -4.83398438e-02,  2.35351562e-01,  1.74804688e-01,\n",
              "       -1.46484375e-01, -7.42187500e-02, -1.01562500e-01, -7.71484375e-02,\n",
              "        1.09375000e-01, -5.71289062e-02, -1.48437500e-01, -6.00585938e-02,\n",
              "        1.74804688e-01, -7.71484375e-02,  2.58789062e-02, -7.66601562e-02,\n",
              "       -3.80859375e-02,  1.35742188e-01,  3.75976562e-02, -4.19921875e-02,\n",
              "       -3.56445312e-02,  5.34667969e-02,  3.68118286e-04, -1.66992188e-01,\n",
              "       -1.17187500e-01,  1.41601562e-01, -1.69921875e-01, -6.49414062e-02,\n",
              "       -1.66992188e-01,  1.00585938e-01,  1.15722656e-01, -2.18750000e-01,\n",
              "       -9.86328125e-02, -2.56347656e-02,  1.23046875e-01, -3.54003906e-02,\n",
              "       -1.58203125e-01, -1.60156250e-01,  2.94189453e-02,  8.15429688e-02,\n",
              "        6.88476562e-02,  1.87500000e-01,  6.49414062e-02,  1.15234375e-01,\n",
              "       -2.27050781e-02,  3.32031250e-01, -3.27148438e-02,  1.77734375e-01,\n",
              "       -2.08007812e-01,  4.54101562e-02, -1.23901367e-02,  1.19628906e-01,\n",
              "        7.44628906e-03, -9.03320312e-03,  1.14257812e-01,  1.69921875e-01,\n",
              "       -2.38281250e-01, -2.79541016e-02, -1.21093750e-01,  2.47802734e-02,\n",
              "        7.71484375e-02, -2.81982422e-02, -4.71191406e-02,  1.78222656e-02,\n",
              "       -1.23046875e-01, -5.32226562e-02,  2.68554688e-02, -3.11279297e-02,\n",
              "       -5.59082031e-02, -5.00488281e-02, -3.73535156e-02,  1.25976562e-01,\n",
              "        5.61523438e-02,  1.51367188e-01,  4.29687500e-02, -2.08007812e-01,\n",
              "       -4.78515625e-02,  2.78320312e-02,  1.81640625e-01,  2.20703125e-01,\n",
              "       -3.61328125e-02, -8.39843750e-02, -3.69548798e-05, -9.52148438e-02,\n",
              "       -1.25000000e-01, -1.95312500e-01, -1.50390625e-01, -4.15039062e-02,\n",
              "        1.31835938e-01,  1.17675781e-01,  1.91650391e-02,  5.51757812e-02,\n",
              "       -9.42382812e-02, -1.08886719e-01,  7.32421875e-02, -1.15234375e-01,\n",
              "        8.93554688e-02, -1.40625000e-01,  1.45507812e-01,  4.49218750e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nJsxPMQkt8l"
      },
      "source": [
        "## 61. 単語の類似度\n",
        "“United States”と”U.S.”のコサイン類似度を計算せよ．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#models.KeyedVectors.similarity(w1,w2) https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.similarity\n",
        "\n",
        "model.similarity('United_States', 'U.S.')"
      ],
      "metadata": {
        "id": "zvjNb6BOnWEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e088719-ca52-4b2a-ee90-bb7157956bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.73107743"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXIt1gJkxFv"
      },
      "source": [
        "## 62. 類似度の高い単語10件"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ"
      ],
      "metadata": {
        "id": "SABOIiqTfbFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#most_similar: https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar\n",
        "model.most_similar('United_States', topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a7XVNl-dxaY",
        "outputId": "53ddaaf7-1a28-4c54-e346-ef177338d8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Unites_States', 0.7877248525619507),\n",
              " ('Untied_States', 0.7541370391845703),\n",
              " ('United_Sates', 0.74007248878479),\n",
              " ('U.S.', 0.7310774326324463),\n",
              " ('theUnited_States', 0.6404393911361694),\n",
              " ('America', 0.6178410053253174),\n",
              " ('UnitedStates', 0.6167312264442444),\n",
              " ('Europe', 0.6132988929748535),\n",
              " ('countries', 0.6044804453849792),\n",
              " ('Canada', 0.6019070148468018)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGJSU-HnkzYs"
      },
      "source": [
        "## 63. 加法構成性によるアナロジー\n",
        "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#most_similar: most_similar([postive terms list], [negative term list], ... other options)\n",
        "model.most_similar(positive=['Spain','Athens'],negative=['Madrid'], topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fwr--9hY8EL",
        "outputId": "300e8499-46ce-4af2-a8b5-c82639107586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Greece', 0.6898481249809265),\n",
              " ('Aristeidis_Grigoriadis', 0.5606848001480103),\n",
              " ('Ioannis_Drymonakos', 0.5552908778190613),\n",
              " ('Greeks', 0.545068621635437),\n",
              " ('Ioannis_Christou', 0.5400862693786621),\n",
              " ('Hrysopiyi_Devetzi', 0.5248444676399231),\n",
              " ('Heraklio', 0.5207759737968445),\n",
              " ('Athens_Greece', 0.516880989074707),\n",
              " ('Lithuania', 0.5166866183280945),\n",
              " ('Iraklion', 0.5146791934967041)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWA6X7WXk2Tr"
      },
      "source": [
        "## 64. アナロジーデータでの実験\n",
        "単語アナロジーの評価データをダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format('./drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "metadata": {
        "id": "LckMNATIGxJO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaVoXvVNo4wd",
        "outputId": "a1f9b895-5b4b-4dc0-fc51-1cab1a7ccba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-25 04:10:02--  http://download.tensorflow.org/data/questions-words.txt\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.12.128, 2607:f8b0:400c:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.12.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603955 (590K) [text/plain]\n",
            "Saving to: ‘questions-words.txt’\n",
            "\n",
            "\rquestions-words.txt   0%[                    ]       0  --.-KB/s               \rquestions-words.txt 100%[===================>] 589.80K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-02-25 04:10:02 (257 MB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## 評価データダウンロード\n",
        "!wget http://download.tensorflow.org/data/questions-words.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwjieCBpLJT"
      },
      "source": [
        "このデータは、(Athens-Greece, Tokyo-Japan)のように、意味的アナロジーを評価するための組と、(walk-walks, write-writes)のように文法的アナロジーを評価する組が含まれる．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVhcAgAxqcJn",
        "outputId": "f68031c3-8932-4fcf-cf61-8e4f58b3d96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": capital-common-countries\n",
            "Athens Greece Baghdad Iraq\n",
            "Athens Greece Bangkok Thailand\n",
            "Athens Greece Beijing China\n",
            "Athens Greece Berlin Germany\n"
          ]
        }
      ],
      "source": [
        "!head questions-words.txt -n 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "file1 = 'questions-words.txt'\n",
        "file2 = 'questions-words_sim.txt'\n",
        "\n",
        "with open(file1,'r') as f1, open(file2,'w') as f2:\n",
        "  for line in tqdm(f1):\n",
        "    line = line.split()\n",
        "    if line[0]==':':\n",
        "      category = line[1]\n",
        "    else:\n",
        "      simW, simV = model.most_similar(positive=[line[1], line[2]], negative=[line[0]], topn=1)[0]#crashes\n",
        "      f2.write(' '.join([category] + line + [simW , str(simV),\"\\n\"]))\n"
      ],
      "metadata": {
        "id": "75J5xWaucJTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdff0e28-50e8-4bbb-8ac4-bda373559050"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19558it [2:06:53,  2.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be_aUKgDk4Th"
      },
      "source": [
        "## 65. アナロジータスクでの正解率\n",
        "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "questions-wordsのデータセットには大きく２種類のカテゴリがある：\n",
        "意味的アナロジー（capital-common-countryなど:Athens Greece Hanoi Vietnam）\n",
        "文法的アナロジー（gram1-adjective-to-adverbなど:happy happily serious seriously）\n",
        "'''\n",
        "syn_all = int() #num of syntactic analogies\n",
        "syn_cor = int() #num of correct syntactic analogies\n",
        "sem_all = int()\n",
        "sem_cor = int()\n",
        "\n",
        "with open('questions-words_sim.txt','r') as f:\n",
        "  for line in f:\n",
        "    line = line.split()\n",
        "    if \"gram\" in line[0]:#syntactic analogy\n",
        "      syn_all += 1\n",
        "      if line[4]==line[5]:\n",
        "        syn_cor += 1\n",
        "    else:\n",
        "      sem_all += 1\n",
        "      if line[4]==line[5]:\n",
        "        sem_cor += 1\n",
        "\n",
        "print(\"semantic analogy accuracy: {}\".format(sem_cor/sem_all))\n",
        "print(\"syntactic analogy accuracy: {}\".format(syn_cor/syn_all))       "
      ],
      "metadata": {
        "id": "LAMBNvGUmEWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2198f9-a97f-431c-d464-e65e4c946d73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semantic analogy accuracy: 0.7308602999210734\n",
            "syntactic analogy accuracy: 0.6801053484602917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-HS-i5Gk6y7"
      },
      "source": [
        "## 66. WordSimilarity-353での評価\n",
        "[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_AFMOSbXWL",
        "outputId": "2c40dd0e-bc7f-4646-830b-3522bcdf180d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-06-09 11:00:13--  http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving www.gabrilovich.com (www.gabrilovich.com)... 208.97.177.37\n",
            "Connecting to www.gabrilovich.com (www.gabrilovich.com)|208.97.177.37|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23257 (23K) [application/zip]\n",
            "Saving to: ‘wordsim353.zip’\n",
            "\n",
            "wordsim353.zip      100%[===================>]  22.71K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-06-09 11:00:13 (269 KB/s) - ‘wordsim353.zip’ saved [23257/23257]\n",
            "\n",
            "Archive:  wordsim353.zip\n",
            "  inflating: combined.csv            \n",
            "  inflating: set1.csv                \n",
            "  inflating: set2.csv                \n",
            "  inflating: combined.tab            \n",
            "  inflating: set1.tab                \n",
            "  inflating: set2.tab                \n",
            "  inflating: instructions.txt        \n"
          ]
        }
      ],
      "source": [
        "!wget http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
        "!unzip wordsim353.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thK_8Z3obiC_",
        "outputId": "120de267-0c99-4453-cc19-831b75d6f574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word 1,Word 2,Human (mean)\r\n",
            "love,sex,6.77\r\n",
            "tiger,cat,7.35\r\n",
            "tiger,tiger,10.00\r\n",
            "book,paper,7.46\r\n",
            "computer,keyboard,7.62\r\n",
            "computer,internet,7.58\r\n",
            "plane,car,5.77\r\n",
            "train,car,6.31\r\n",
            "telephone,communication,7.50\r\n"
          ]
        }
      ],
      "source": [
        "!head  './combined.csv' -n 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGcTAvCmk9Ay"
      },
      "source": [
        "## 67. k-meansクラスタリング\n",
        "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOYVR3MXk_yQ"
      },
      "source": [
        "## 68. Ward法によるクラスタリング\n",
        "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn41cF2VlBcG"
      },
      "source": [
        "## 69. t-SNEによる可視化\n",
        "ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ．"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Chapter07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}